#
# GPL HEADER START
#
# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 only,
# as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License version 2 for more details (a copy is included
# in the LICENSE file that accompanied this code).
#
# You should have received a copy of the GNU General Public License
# version 2 along with this program; If not, see http://www.gnu.org/licenses
#
# Please  visit http://www.xyratex.com/contact if you need additional
# information or have any questions.
#
# GPL HEADER END
#
# Copyright 2012 Xyratex Technology Limited
#
# Author: Roman Grigoryev<Roman_Grigoryev@xyratex.com>
#
=pod

=head1 Xperior harness user guide

=head2 L<Xperior::Executor::Noop> executor and simple run

Xpreior harness have basic Noop test executor which could be use for proving
installation and Xperior investigation. Simple comand line for execute
sample empty tests:

    bin/xper --debug --workdir /tmp/demo --testdir=t/testcfgs/simple \
        --config=t/testcfgs/localtestsystemcfg.yaml --html

By default, C<t/testcfgs/localtestsystemcfg.yaml> use B<local ssh> and user
B<tomcat>.
I<Please add local user tomcat with passwordless ssh or replace
user tomcat to other user which have passwordless local access to exists
user with passwordless login>. More about L<xper|runtest> and its
parameters see on it page. More about configuration see on current page below.

Simple check for test descriptor files could be done via L<checkyaml> tool,
e.g.

    bin/checkyaml.pl --ydir testds/

Other executors could provide support for different test, e.g.
L<Xperior::Executor::IOR> or L<Xperior::Executor::LustreTests>


=head2  L<Xperior::Executor::LustreTests> and quick Lustre test run

This chapter describes executing few Lustre sanity tests on one-node
Lustre setup when all components include client are on one node.

I<Instruction tested for case when Xperior executed on real node (B<xnode>)
and Lustre one-node cluster in virtual machine (B<lnode>)>

I<This setup should not be used for regular execution,
see L<XperiorIntegrationNotes> for more details>

=over 12

=item 1. Nodes preparation

B<Xnode> must be able to do passwordless ssh connection to B<lnode >
with root or administrator user. B<Lnode> must be able to locally ssh
on its public address. User, which should be used for connect, and
external B<lnode> address should be defined in system configuration,
see item 3.

If you use host names please check that names could be resolved by
nodes via dns or /etc/hosts.

=item 2. Create system configuration file.

Basic sample see  L<there|"System configuration for basic one node cluster">.
You can use copy it and save on disk as configuration for one node cluster,
don't forget check B<user> and B<ip> fields. Use created file name in
I<--config> option on next step.

More details about configuration see L<there|"System descriptor">

=item 3. Run Xperior

 bin/runtest.pl --workdir='/tmp/wd' --testdir=testds \
 --config=new_config_file.yaml  --includeonly='sanity/0.*' --action=run --html

You could use C<--debug> for see verbose messages. After end of execution
see html report in C</tmp/wd> (or value of I<--workdir> option).

=back

=head2 Xperior system configuration examples

=over

=item 1. System configuration for basic one node cluster

One mgs/mds/oss combined node with one mdt and two ost's.
One master client and one remote client both are runing
on the same host above.

 ---
 Nodes:
   - id           : lnode
     ip           : 192.168.0.10
     ctrlproto    : ssh
     user         : root
     nodetype     : BasicNode

 LustreObjects:
   - id          : mds1
     device      : /tmp/lustre-mdt1
     node        : lnode
     type        : mds

   - id          : oss1
     device      : /tmp/lustre-ost1
     node        : node
     type        : oss

   - id          : oss2
     device      : /tmp/lustre-ost2
     node        : lnode
     type        : oss

   - id          : client1
     node        : lnode
     type        : client
     master      : yes

   - id          : client2
     node        : lnode
     type        : client

 #Variables
 DefaultLustreClient:
     master      : no
     mount_point : /mnt/lustre
 DefaultNode:
     ctrlproto : ssh
     user      : root
 lustre_mount_point      : /mnt/lustre
 lustre_device_type      : loop   
 client_mount_point      : /mnt/lustre
 benchmark_tests_file    : testfile
 tempdir                 : /tmp

=item 2. Failover configuration example

One failover pair of one target combined mgs/mds node in active-passive mode.

One failover pair of two target oss nodes in active-active mode.

This configuration targeted to IB network

 ---
 Nodes:
   - id           : mgs-mds-1
     ip           : vmhost1
     nodetype     : KVMNode
     console      : /tmp/serial-vmhost1.log
     kvmdomain    : vmhost1

   - id           : mgs-mds-2
     ip           : vmhost2
     nodetype     : KVMNode
     console      : /tmp/serial-vmhost2.log
     kvmdomain    : vmhost2

   - id           : oss-1
     ip           : vmhost3
     nodetype     : KVMNode
     console      : /tmp/serial-vmhost3.log
     kvmdomain    : vmhost3

   - id           : oss-2
     ip           : vmhost4
     nodetype     : KVMNode
     console      : /tmp/serial-vmhost4.log
     kvmdomain    : vmhost4

   - id           : master-client
     ip           : vmhost5
     nodetype     : KVMNode
     console      : /tmp/serial-vmhost5.log
     kvmdomain    : vmhost5


 LustreObjects:
   - id          : mdt1
     device      : /tmp/lustre-mdt1
     node        : mgs-mds-1
     failover    : mgs-mds-2
     type        : mds

   - id          : ost1
     device      : /tmp/lustre-ost1
     node        : oss-1
     failover    : oss-2
     type        : oss

   - id          : ost2
     device      : /tmp/lustre-ost2
     node        : oss-2
     failover    : oss-1
     type        : oss

   - id          : master-client
     node        : master-client
     type        : client
     master      : yes

 #Variables
 nettype                 : o2ib
 client_mount_point      : /mnt/lustre
 benchmark_tests_file    : testfile
 tempdir                 : /tmp

=item 3. Clientonly configuration example

This configuration should be used for setups where only
client available for actions, e.g. on ready clusters with HA

---
Nodes:

   - id           : cng0
     ip           : kvm1n1c006
     ctrlproto    : ssh
     user         : root
     nodetype     : Basic

   - id           : cng1
     ip           : kvm1n1c007
     ctrlproto    : ssh
     user         : root
     nodetype     : Basic


LustreObjects:


   - id          : client
     node        : cng0
     type        : client
     master      : yes

   - id          : client
     node        : cng1
     type        : client




#Variables
mgsnid        : 192.168.3.12@tcp:192.168.3.13@tcp:/testfs
clientonly              : 1
client_mount_point      : /mnt/testfs
benchmark_tests_file    : testfile
tempdir                 : /
#/tmp/


=back

=head2 Input/output data for Xperior

Xperior expects on input L<system configuration|"System descriptor"> and
directory with L<test descriptors|"Test descriptor">. After start, Xperior
does checks for nodes and servcies which defined in configuration (see
L<Xperior::Node>). Further, Xperior iterate over described tests and execute
every test. Execution results are stored in work directory in
L<xprerior result files |"Description of YAML test result">.

=head3 Test descriptor

Test descriptor file must be placed in test directory (see Synopsis section).
It must be yaml file and have C<_tests.yaml> end of name..

Sample test descriptor 1:
 ---
 groupname        : ior
 executor         : Xperior::Executor::IOR
 description      : IOR tests
 reference        : http://wiki.lustre.org/index.php/Testing_Lustre_Code
 expected_time    : 60
 timeout          : 300
 cleanup_max_time : -1
 iorcmd           : /usr/bin/IOR -a POSIX -i 5 -C -g -v -e -w -r -b 10m -t 4k -o @mount_pointE@/@test_fileE@
 tags             : benchmark ior
 Tests:
    - id               : test1


Sample test descriptor 2

 ---
 groupname        : sanity
 executor         : Xperior::Executor::LustreTests
 roles            : StoreSyslog StoreConsole GetDiagnostics
 description      : Lustre sanity tests
 reference        : http://wiki.lustre.org/index.php/Testing_Lustre_Code
 expected_time    : 10
 timeout          : 300
 cleanup_max_time : -1
 #options          :
 tags             : functional
 dangerous        : yes
 # Per test environment variables
 Env:
    POWER_UP        : "ssh root@kvmhost virsh start"
    POWER_DOWN      : "ssh root@kvmhost virsh destroy"
    FAILURE_MODE    : HARD
 Tests:
    - id               : 0b
    - id               : 0c
    - id               : 1a
    - id               : 1b

The file contains set of tags. Main entity is test, in this sample it
is single element C<id> is C<Tests> array. Every test inherit vales from
common description (fields which  described out of C<Tests> array). A test
can override any field or add new fields. Supported fields divided into 2
groups: common fields and executor-specific fields. See alone executor
specific field C<iorcmd>  in explanation below.

B<groupname>, B<executor>, B<description>, B<reference>, B<roles>, B<tags>
- are common fields. All other are executor-specific and used in executors.


=over

=item *

C<groupname>        - name of tests group. Used as directory for test
results placement.

=item *

C<executor>         - full qualified perl module name which implements
Executor interface.

=item *

C<descriptor> ,C<reference> - self-documentation tags

=item *

C<expected_time>    - expected time for test execution. If actual test
execution need more time this will be pointed in result. Not implemented.

=item *

C<timeout>          - after this time execution will be killed

=item *

C<cleanup_max_time> - timeout for cleanup system for next test. Not
implemented.

=item *

C<iorcmd>           - command for executing, specific parameter for
Xperior::Executor::IOR. C<@mount_pointE@> and C<@test_fileE@> are
variables from system configuration, see their definition in next section.

=item *

C<tags>             - test tags (labels) space separated list, used for
simple test filtering. See also C<testds/tags.yaml>.

=item *

C<Tests:>           - array of single tests

=item *

C<id>               - test id. Obligatory tests attribute which cannot be
inherit.

=back

=head3 System descriptor

B<Warning!> The system descriptor is not ended, it is possible that in
future new test executor will need more info about system.

Descriptor is yaml file, which have 2 parts: C<Nodes> array, C<LustreObjects>
and system wide variables. TBD.

 Nodes:
   - id          : mds1
     ip          : 192.168.200.102
     ctrlproto   : ssh
     user        : root
........................
   - id           : client1
     ip           : 192.168.200.150
     ctrlproto    : ssh
     user         : root
LustreObjects:
   - id          : mds1
     device      : /dev/sda1
     node        : mds1
     type        : mds

   - id          : oos1
     device      : /dev/sda1
     node        : oss1
     type        : oss

   - id          : client1
     node        : client1
     type        : client
     master      : yes

   - id          : client2
     node        : mds1
     type        : client

 #Variables
client_mount_point      : /mnt/lustre/
benchmark_tests_file    : testfile

=head3 Description of YAML test result

Sample YAML result from lustre  C<sanity/101b>  test with comments.
Also can be generated tap and html reports.

 ---
 cleanup_max_time: -1                                 # not used now
 cmd: SLOW=YES  MDSCOUNT=1 MDSDEV1=/tmp/lustre-mdt1 mds1_HOST=mds
  OSTCOUNT=2 OSTDEV1=/tmp/lustre-ost1 ost1_HOST=mds  OSTDEV2=/tmp/lustre-ost2
  ost2_HOST=mds CLIENTS=lclient RCLIENTS=\"mds\" ONLY=101b DIR=/mnt/lustre/
  PDSH=\"/usr/bin/pdsh -S -w \" /usr/lib64/lustre/tests/sanity.sh 2>
  /tmp/test_stderr.569020.log 1>  /tmp/test_stdout.569020.log # comman which will be executed
 completed: yes  #test execution completed
 dangerous: yes
 description: Lustre sanity tests
 endtime: 1328712096                                   # unix epoch when test execution completed
 endtime_planned: 1328712332                           # time when execution will be interrupted by timeout
 executor: Xperior::Executor::LustreTests
 exitcode: 0                                           # exit code of 'cmd' program
 expected_time: 10                                     # not used now
 groupname: sanity                                     # test group name
 id: 101b                                              # test id, main test identificator
 killed: no                                            # was 'cmd' killed by timeout or not
 log:                                                  # array of log
   console.mds1: 101b.console.mds1.log
   messages.client2: 101b.messages.client2.log
   messages.mds1: 101b.messages.mds1.log
   messages.oss1: 101b.messages.oss1.log
   messages.oss2: 101b.messages.oss2.log
   stderr: 101b.stderr.log
   stdout: 101b.stdout.log
 masterclient:                                         # node where test started
   id: client1
   master: yes
   node: client1
   type: client
 messages: |                                           # messages from xperior
   No console defined for node [oss1]
   No console defined for node [oss2]
   No console defined for node [client1]
   No console defined for node [client2]
   Cannot copy log file [/tmp/messageslog.1328712015.34009]: 256
 reference: http://wiki.lustre.org/index.php/Testing_Lustre_Code
 result: 'ok 1 '                                        # tap test string result
 roles: StoreSyslog StoreConsole
 schema: Xperior1                                        # yaml schema id
 starttime: 1328712032                                   # unix epoch when test execution started
 status: passed                                          # test execution result as string
 status_code: 0                                          # test execution result code
 tags: functional
 timeout: 300


=head2 See also


Many ideas get from L<http://en.wikipedia.org/wiki/Test_Anything_Protocol>
and perl test framework based on TAP. Internally harness use yaml part of
tap and also can export full TAP output.

Yaml specification there: L<http://www.yaml.org/>


=cut

