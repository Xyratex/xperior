==> /tmp/serial_mft02_out <==
Lustre: DEBUG MARKER: only running test 0b
Lustre: DEBUG MARKER: excepting tests: 15c
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: 4165:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from 1b7c9b0f-22e5-a058-7766-039e5320f0d9@0@lo t0 exp (null) cur 1333466062 last 0
Lustre: 4165:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 2 previous similar messages
Lustre: Client lustre-client has started
Lustre: DEBUG MARKER: Using TIMEOUT=20
Lustre: 4165:0:(quota_master.c:793:close_quota_files()) quota[0] is off already
Lustre: 4165:0:(quota_master.c:793:close_quota_files()) Skipped 1 previous similar message
LustreError: 5797:0:(quota_ctl.c:328:client_quota_ctl()) ptlrpc_queue_wait failed, rc: -114
Lustre: DEBUG MARKER: == replay-dual test 0b: lost client during waiting for next transno == 18:14:33 (1333466073)
LustreError: 6104:0:(osd_handler.c:938:osd_ro()) *** setting device osd-ldiskfs read-only ***
Turning device loop0 (0x700000) read-only
Lustre: DEBUG MARKER: mds1 REPLAY BARRIER on lustre-MDT0000
Lustre: DEBUG MARKER: local REPLAY BARRIER on lustre-MDT0000
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd408
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdcf8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdc90
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdc28
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdbc0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdb58
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdaf0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bda88
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bda20
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bdcf8
LustreError: 6115:0:(ldlm_request.c:1172:ldlm_cli_cancel_req()) Got rc -108 from cancel RPC: canceling anyway
LustreError: 6115:0:(ldlm_request.c:1799:ldlm_cli_cancel_list()) ldlm_cli_cancel_list: -108
Lustre: client ffff880075f85c00 umount complete
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797de880
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd8e8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd6e0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd678
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd610
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd5a8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd540
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd470
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797960c8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880079796130
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880079796198
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007b0bd8e8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007680b818
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d15a8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797de6e0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007680b9b8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007680b5a8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d1748
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d1880
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797de8e8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797de950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d17b0
Lustre: Failing over lustre-MDT0000
Lustre: Skipped 3 previous similar messages
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880079796268
Lustre: 6151:0:(quota_master.c:793:close_quota_files()) quota[0] is off already
Lustre: 6151:0:(quota_master.c:793:close_quota_files()) Skipped 1 previous similar message
Lustre: mdd_obd-lustre-MDT0000: shutting down for failover; client state will be preserved.
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880079796338
Lustre: MGS has stopped.
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007680b7b0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007680b610
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007680b7b0
Removing read-only on unknown block (0x700000)
Lustre: server umount lustre-MDT0000 complete
LDISKFS-fs warning (device loop0): ldiskfs_fill_super: extents feature not enabled on this filesystem, use tune2fs.
LDISKFS-fs (loop0): recovery complete
LDISKFS-fs (loop0): mounted filesystem with ordered data mode
LDISKFS-fs warning (device loop0): ldiskfs_fill_super: extents feature not enabled on this filesystem, use tune2fs.
LDISKFS-fs (loop0): mounted filesystem with ordered data mode
Lustre: MGS MGS started
LustreError: 166-1: MGC10.3.0.188@tcp: Connection to service MGS via nid 0@lo was lost; in progress operations using this service will fail.
Lustre: MGC10.3.0.188@tcp: Reactivating import
Lustre: Skipped 1 previous similar message
Lustre: 6267:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import MGS->NET_0x9000000000000_UUID netid 90000: select flavor null
Lustre: 6267:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 11 previous similar messages
Lustre: Enabling ACL
Lustre: Enabling user_xattr
Lustre: lustre-MDT0000: used disk, loading
Lustre: 6271:0:(ldlm_lib.c:1900:target_recovery_init()) RECOVERY: service lustre-MDT0000, 2 recoverable clients, last_transno 8589934644
LustreError: 6275:0:(ldlm_lib.c:1737:target_recovery_thread()) lustre-MDT0000: started recovery thread pid 6275
Lustre: 6271:0:(mdt_lproc.c:259:lprocfs_wr_identity_upcall()) lustre-MDT0000: identity upcall set to /usr/sbin/l_getidentity
Lustre: 6271:0:(mds_lov.c:1004:mds_notify()) MDS mdd_obd-lustre-MDT0000: add target lustre-OST0000_UUID
Lustre: 6271:0:(mds_lov.c:1004:mds_notify()) Skipped 1 previous similar message
Lustre: 2306:0:(ldlm_lib.c:802:target_handle_connect()) lustre-OST0000: received new MDS connection from NID 0@lo, removing former export from same NID
Lustre: 2306:0:(filter.c:2710:filter_connect_internal()) lustre-OST0000: Received MDS connection for group 0
Lustre: 2306:0:(filter.c:2710:filter_connect_internal()) Skipped 1 previous similar message
Lustre: 1615:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0000_UUID
Lustre: 1615:0:(mds_lov.c:1024:mds_notify()) Skipped 1 previous similar message
Lustre: 6307:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 6307:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 5 previous similar messages
Lustre: setting import lustre-MDT0000_UUID INACTIVE by administrator request
Lustre: Skipped 1 previous similar message
Lustre: client ffff88007818f400 umount complete
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 60s
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880078a80800 x1398240341656288/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466107 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 55s
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880076762800 x1398240341656301/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466112 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
Lustre: 6277:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from 267f2418-c530-512b-b7d9-e43d69375cb0@0@lo recovering/t0 exp (null) cur 1333466097 last 0
Lustre: 6277:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 7 previous similar messages
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 49s
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880076771000 x1398240341656303/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466117 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 45s
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880077e2f000 x1398240341656307/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466122 ref 1 fl Interpret:/0/0 rc -16/0
Lustre: lustre-OST0001: haven't heard from client 98f9ac1c-d14e-895e-9783-ce999d1490b8 (at 0@lo) in 59 seconds. I think it's dead, and I am evicting it. exp ffff880075d15c00, cur 1333466102 expire 1333466072 last 1333466043
Lustre: lustre-OST0000: haven't heard from client 98f9ac1c-d14e-895e-9783-ce999d1490b8 (at 0@lo) in 59 seconds. I think it's dead, and I am evicting it. exp ffff880075d10800, cur 1333466102 expire 1333466072 last 1333466043
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 40s
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff8800765e3000 x1398240341656311/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466127 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 35s
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880077e2f000 x1398240341656318/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466137 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) Skipped 1 previous similar message
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: Skipped 1 previous similar message
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 24s
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) Skipped 1 previous similar message
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880079f0e800 x1398240341656334/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1333466157 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 6277:0:(ldlm_lib.c:2125:target_send_reply_msg()) Skipped 3 previous similar messages
Lustre: lustre-OST0001: haven't heard from client 93c77f93-50ea-22f9-43db-321ba40b0068 (at 0@lo) in 55 seconds. I think it's dead, and I am evicting it. exp ffff880037b3d400, cur 1333466137 expire 1333466107 last 1333466082
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: Skipped 3 previous similar messages
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (267f2418-c530-512b-b7d9-e43d69375cb0): 0 clients in recovery for 4s
LustreError: 6277:0:(ldlm_lib.c:906:target_handle_connect()) Skipped 3 previous similar messages
Lustre: 6275:0:(ldlm_lib.c:1566:target_recovery_overseer()) recovery is timed out, evict stale exports
LustreError: 6275:0:(genops.c:1273:class_disconnect_stale_exports()) lustre-MDT0000: disconnect stale client 1b7c9b0f-22e5-a058-7766-039e5320f0d9@<unknown>
LustreError: 6275:0:(genops.c:1273:class_disconnect_stale_exports()) lustre-MDT0000: disconnect stale client 93c77f93-50ea-22f9-43db-321ba40b0068@<unknown>
Lustre: lustre-MDT0000: sending delayed replies to recovered clients
Lustre: 6275:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0000_UUID
Lustre: 6275:0:(mds_lov.c:1024:mds_notify()) Skipped 1 previous similar message
Lustre: lustre-OST0001: received MDS connection from 0@lo
Lustre: Skipped 1 previous similar message
Lustre: 2306:0:(lustre_log.h:471:llog_group_set_export()) lustre-OST0001: export for group 0 is changed: 0xffff880079305c00 -> 0xffff88007a49a800
Lustre: 2306:0:(lustre_log.h:471:llog_group_set_export()) Skipped 2 previous similar messages
Lustre: 2306:0:(llog_net.c:168:llog_receptor_accept()) changing the import ffff8800792fb000 - ffff880079f0e000
Lustre: 2306:0:(llog_net.c:168:llog_receptor_accept()) Skipped 2 previous similar messages
Lustre: 2308:0:(filter.c:2566:filter_llog_connect()) lustre-OST0000: Recovery from log 0x1f/0x0:f004dbbe
Lustre: MDS mdd_obd-lustre-MDT0000: lustre-OST0000_UUID now active, resetting orphans
Lustre: Skipped 1 previous similar message
Lustre: 2308:0:(filter.c:2566:filter_llog_connect()) lustre-OST0001: Recovery from log 0x20/0x0:f004dbbf
Lustre: 6277:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import lustre-MDT0000->NET_0x9000000000000_UUID netid 90000: select flavor null
Lustre: 6277:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 5 previous similar messages
Lustre: Client lustre-client has started
Lustre: 6512:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 6512:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 1 previous similar message
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: Client lustre-client has started
Lustre: DEBUG MARKER: == replay-dual replay-dual.sh test complete, duration 96 sec == 18:15:58 (1333466158)
LustreError: 6758:0:(ldlm_request.c:1172:ldlm_cli_cancel_req()) Got rc -108 from cancel RPC: canceling anyway
LustreError: 6758:0:(ldlm_request.c:1799:ldlm_cli_cancel_list()) ldlm_cli_cancel_list: -108
Lustre: client ffff88007a19c800 umount complete
