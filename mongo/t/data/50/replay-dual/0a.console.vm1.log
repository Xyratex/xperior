==> /tmp/serial_mft02_out <==
Lustre: DEBUG MARKER: only running test 0a
Lustre: DEBUG MARKER: excepting tests: 15c
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: 3270:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import lustre-MDT0000-mdc-ffff88007c2e7000->10.3.0.188@tcp netid 90000: select flavor null
Lustre: 3270:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 4 previous similar messages
Lustre: 2221:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from 98f9ac1c-d14e-895e-9783-ce999d1490b8@0@lo t0 exp (null) cur 1333465940 last 0
Lustre: 2221:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 2 previous similar messages
Lustre: Client lustre-client has started
Lustre: 3289:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 3289:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 1 previous similar message
Lustre: DEBUG MARKER: Using TIMEOUT=20
Lustre: 2221:0:(quota_master.c:793:close_quota_files()) quota[0] is off already
Lustre: 2221:0:(quota_master.c:793:close_quota_files()) Skipped 1 previous similar message
LustreError: 3674:0:(quota_ctl.c:328:client_quota_ctl()) ptlrpc_queue_wait failed, rc: -114
Lustre: DEBUG MARKER: == replay-dual test 0a: expired recovery with lost client == 18:12:32 (1333465952)
LustreError: 3994:0:(osd_handler.c:938:osd_ro()) *** setting device osd-ldiskfs read-only ***
Turning device loop0 (0x700000) read-only
Lustre: DEBUG MARKER: mds1 REPLAY BARRIER on lustre-MDT0000
Lustre: DEBUG MARKER: local REPLAY BARRIER on lustre-MDT0000
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d48e8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d49b8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4a20
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4a88
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4af0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4b58
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4bc0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4c28
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4c90
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4cf8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4d60
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4dc8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4e30
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4e98
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d10c8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d05a8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d14d8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d2b58
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d2af0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d2cf8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4200
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4268
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4470
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4610
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d1f00
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d03a0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880079566c28
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880079566bc0
Lustre: Failing over lustre-MDT0000
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d49b8
Lustre: 4041:0:(quota_master.c:793:close_quota_files()) quota[0] is off already
Lustre: 4041:0:(quota_master.c:793:close_quota_files()) Skipped 1 previous similar message
Lustre: Failing over mdd_obd-lustre-MDT0000
Lustre: mdd_obd-lustre-MDT0000: shutting down for failover; client state will be preserved.
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800797d4a88
Lustre: MGS has stopped.
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795ad950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795d20c8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff8800795ad950
Removing read-only on unknown block (0x700000)
Lustre: server umount lustre-MDT0000 complete
Lustre: 1614:0:(client.c:1773:ptlrpc_expire_one_request()) @@@ Request  sent has timed out for slow reply: [sent 1333465955/real 1333465955]  req@ffff8800790ca800 x1398240341655848/t0(0) o400->MGC10.3.0.188@tcp@0@lo:26/25 lens 192/192 e 0 to 1 dl 1333465962 ref 1 fl Rpc:XN/0/ffffffff rc 0/-1
LustreError: 166-1: MGC10.3.0.188@tcp: Connection to service MGS via nid 0@lo was lost; in progress operations using this service will fail.
LDISKFS-fs warning (device loop0): ldiskfs_fill_super: extents feature not enabled on this filesystem, use tune2fs.
LDISKFS-fs (loop0): recovery complete
LDISKFS-fs (loop0): mounted filesystem with ordered data mode
LDISKFS-fs warning (device loop0): ldiskfs_fill_super: extents feature not enabled on this filesystem, use tune2fs.
LDISKFS-fs (loop0): mounted filesystem with ordered data mode
Lustre: MGS MGS started
Lustre: 1614:0:(client.c:1773:ptlrpc_expire_one_request()) @@@ Request  sent has timed out for slow reply: [sent 1333465960/real 1333465960]  req@ffff88007b50e000 x1398240341655850/t0(0) o400->lustre-MDT0000-mdc-ffff88007818f400@0@lo:12/10 lens 192/192 e 0 to 1 dl 1333465967 ref 1 fl Rpc:XN/0/ffffffff rc 0/-1
Lustre: lustre-MDT0000-mdc-ffff88007818f400: Connection to service lustre-MDT0000 via nid 0@lo was lost; in progress operations using this service will wait for recovery to complete.
Lustre: lustre-MDT0000-mdc-ffff88007c2e7000: Connection to service lustre-MDT0000 via nid 0@lo was lost; in progress operations using this service will wait for recovery to complete.
LustreError: 4137:0:(import.c:326:ptlrpc_invalidate_import()) MGS: rc = -110 waiting for callback (1 != 0)
LustreError: 4137:0:(import.c:352:ptlrpc_invalidate_import()) @@@ still on sending list  req@ffff88007bde6800 x1398240341655856/t0(0) o250->MGC10.3.0.188@tcp@0@lo:26/25 lens 368/512 e 0 to 0 dl 1333465968 ref 1 fl Rpc:N/0/ffffffff rc 0/-1
LustreError: 4137:0:(import.c:368:ptlrpc_invalidate_import()) MGS: RPCs in "Unregistering" phase found (0). Network is sluggish? Waiting them to error out.
Lustre: 1615:0:(client.c:1773:ptlrpc_expire_one_request()) @@@ Request  sent has timed out for slow reply: [sent 1333465962/real 1333465962]  req@ffff88007bde6800 x1398240341655856/t0(0) o250->MGC10.3.0.188@tcp@0@lo:26/25 lens 368/512 e 0 to 1 dl 1333465968 ref 1 fl Rpc:XN/0/ffffffff rc 0/-1
Lustre: 1615:0:(client.c:1773:ptlrpc_expire_one_request()) Skipped 1 previous similar message
Lustre: MGC10.3.0.188@tcp: Reactivating import
Lustre: 4137:0:(import.c:526:import_select_connection()) MGC10.3.0.188@tcp: tried all connections, increasing latency to 6s
Lustre: 4146:0:(ldlm_lib.c:873:target_handle_connect()) MGS: connection from 6f14272c-4478-4cff-758c-9b5b9e705419@0@lo t0 exp (null) cur 1333465968 last 0
Lustre: 4146:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 2 previous similar messages
Lustre: 4146:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import MGS->NET_0x9000000000000_UUID netid 90000: select flavor null
Lustre: 4146:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 5 previous similar messages
Lustre: Enabling ACL
Lustre: Enabling user_xattr
Lustre: lustre-MDT0000: used disk, loading
Lustre: 4160:0:(ldlm_lib.c:1900:target_recovery_init()) RECOVERY: service lustre-MDT0000, 2 recoverable clients, last_transno 4294967299
LustreError: 4164:0:(ldlm_lib.c:1737:target_recovery_thread()) lustre-MDT0000: started recovery thread pid 4164
Lustre: 4160:0:(mdt_lproc.c:259:lprocfs_wr_identity_upcall()) lustre-MDT0000: identity upcall set to /usr/sbin/l_getidentity
Lustre: 4160:0:(mds_lov.c:1004:mds_notify()) MDS mdd_obd-lustre-MDT0000: add target lustre-OST0000_UUID
Lustre: 2308:0:(ldlm_lib.c:802:target_handle_connect()) lustre-OST0000: received new MDS connection from NID 0@lo, removing former export from same NID
Lustre: 2308:0:(filter.c:2710:filter_connect_internal()) lustre-OST0000: Received MDS connection for group 0
Lustre: 1615:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0000_UUID
Lustre: 2308:0:(ldlm_lib.c:802:target_handle_connect()) lustre-OST0001: received new MDS connection from NID 0@lo, removing former export from same NID
Lustre: 1615:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0001_UUID
Lustre: 4196:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 4196:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 3 previous similar messages
Lustre: 1615:0:(client.c:1773:ptlrpc_expire_one_request()) @@@ Request  sent has timed out for slow reply: [sent 1333465967/real 1333465967]  req@ffff8800779e6800 x1398240341655863/t0(0) o38->lustre-MDT0000-mdc-ffff88007818f400@0@lo:12/10 lens 368/512 e 0 to 1 dl 1333465973 ref 1 fl Rpc:XN/0/ffffffff rc 0/-1
Lustre: 1616:0:(import.c:526:import_select_connection()) lustre-MDT0000-mdc-ffff88007818f400: tried all connections, increasing latency to 6s
Lustre: 4166:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from 93c77f93-50ea-22f9-43db-321ba40b0068@0@lo recovering/t4294967301 exp ffff88007a457000 cur 1333465978 last 1333465968
Lustre: 4166:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 2 previous similar messages
Lustre: 4168:0:(ldlm_lib.c:2026:target_queue_recovery_request()) Next recovery transno: 4294967300, current: 4294967300, replaying
LustreError: 1615:0:(client.c:2585:ptlrpc_replay_interpret()) @@@ status -116, old was 0  req@ffff880075d08000 x1398240341655680/t4294967300(4294967300) o35->lustre-MDT0000-mdc-ffff88007c2e7000@0@lo:23/10 lens 360/424 e 0 to 0 dl 1333465990 ref 2 fl Interpret:R/4/0 rc -116/-116
Lustre: 4165:0:(ldlm_lib.c:2026:target_queue_recovery_request()) Next recovery transno: 4294967301, current: 4294967303, replaying
LustreError: 1615:0:(libcfs_fail.h:81:cfs_fail_check_set()) *** cfs_fail_loc=514 ***
Lustre: lustre-MDT0000-mdc-ffff88007c2e7000: Connection restored to service lustre-MDT0000 using nid 0@lo.
Lustre: 4164:0:(ldlm_lib.c:1566:target_recovery_overseer()) recovery is timed out, evict stale exports
LustreError: 4164:0:(genops.c:1273:class_disconnect_stale_exports()) lustre-MDT0000: disconnect stale client 98f9ac1c-d14e-895e-9783-ce999d1490b8@0@lo
Lustre: 4164:0:(ldlm_lib.c:1819:target_recovery_thread()) too long recovery - read logs
Lustre: lustre-MDT0000-mdc-ffff88007818f400: Connection restored to service lustre-MDT0000 using nid 0@lo.
Lustre: lustre-MDT0000: sending delayed replies to recovered clients
LustreError: dumping log to /tmp/lustre-log.1333466044.4164
Lustre: setting import lustre-MDT0000_UUID INACTIVE by administrator request
Lustre: setting import lustre-OST0000_UUID INACTIVE by administrator request
Lustre: 4164:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0000_UUID
Lustre: client ffff88007c2e7000 umount complete
Lustre: lustre-OST0001: received MDS connection from 0@lo
Lustre: 2306:0:(lustre_log.h:471:llog_group_set_export()) lustre-OST0001: export for group 0 is changed: 0xffff880077a27400 -> 0xffff880079305c00
Lustre: 2306:0:(llog_net.c:168:llog_receptor_accept()) changing the import ffff880077a7d000 - ffff8800792fb000
Lustre: 2306:0:(lustre_log.h:471:llog_group_set_export()) lustre-OST0001: export for group 1 is changed: 0xffff880077a27400 -> 0xffff880079305c00
Lustre: 2306:0:(llog_net.c:168:llog_receptor_accept()) changing the import ffff880077a7d000 - ffff8800792fb000
Lustre: MDS mdd_obd-lustre-MDT0000: lustre-OST0000_UUID now active, resetting orphans
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: 4416:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import lustre-MDT0000-mdc-ffff880075d0c400->10.3.0.188@tcp netid 90000: select flavor null
Lustre: 4416:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 6 previous similar messages
Lustre: 4165:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from ee594b0a-5614-5d85-dd1e-f6ccd169cca0@0@lo t0 exp (null) cur 1333466045 last 0
Lustre: 4165:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 1 previous similar message
Lustre: Client lustre-client has started
Lustre: 4429:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 4429:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 1 previous similar message
Lustre: DEBUG MARKER: == replay-dual replay-dual.sh test complete, duration 106 sec == 18:14:06 (1333466046)
LustreError: 4635:0:(ldlm_request.c:1172:ldlm_cli_cancel_req()) Got rc -108 from cancel RPC: canceling anyway
LustreError: 4635:0:(ldlm_request.c:1799:ldlm_cli_cancel_list()) ldlm_cli_cancel_list: -108
Lustre: client ffff880075d0c400 umount complete
