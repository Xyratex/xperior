==> /tmp/serial_mft01_out <==
Lustre: DEBUG MARKER: only running test 0b
Lustre: DEBUG MARKER: excepting tests: 15c
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: 4229:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from 07ae78f5-2f83-cc97-9bb3-c24019145e36@0@lo t0 exp (null) cur 1331863035 last 0
Lustre: 4229:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 2 previous similar messages
Lustre: Client lustre-client has started
Lustre: DEBUG MARKER: Using TIMEOUT=20
Lustre: 4229:0:(quota_master.c:793:close_quota_files()) quota[0] is off already
Lustre: 4229:0:(quota_master.c:793:close_quota_files()) Skipped 1 previous similar message
LustreError: 5786:0:(quota_ctl.c:328:client_quota_ctl()) ptlrpc_queue_wait failed, rc: -114
Lustre: DEBUG MARKER: == replay-dual test 0b: lost client during waiting for next transno == 03:57:27 (1331863047)
LustreError: 6093:0:(osd_handler.c:938:osd_ro()) *** setting device osd-ldiskfs read-only ***
Turning device loop0 (0x700000) read-only
Lustre: DEBUG MARKER: mds1 REPLAY BARRIER on lustre-MDT0000
Lustre: DEBUG MARKER: local REPLAY BARRIER on lustre-MDT0000
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611bb58
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611baf0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611ba88
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611ba20
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b9b8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b8e8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b880
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b818
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611baf0
LustreError: 6104:0:(ldlm_request.c:1172:ldlm_cli_cancel_req()) Got rc -108 from cancel RPC: canceling anyway
LustreError: 6104:0:(ldlm_request.c:1799:ldlm_cli_cancel_list()) ldlm_cli_cancel_list: -108
Lustre: client ffff880076d94c00 umount complete
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078da8338
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b748
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b6e0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b470
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b408
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b3a0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610c0c8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610c130
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610c198
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610cf68
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610cf00
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610ce98
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b6e0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078d6e950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078da8408
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc2f00
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc0e30
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc0c28
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc2950
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078da8470
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc2d60
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc2dc8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078da87b0
Lustre: Failing over lustre-MDT0000
Lustre: Skipped 3 previous similar messages
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007611b3a0
Lustre: 6152:0:(quota_master.c:793:close_quota_files()) quota[0] is off already
Lustre: 6152:0:(quota_master.c:793:close_quota_files()) Skipped 1 previous similar message
Lustre: mdd_obd-lustre-MDT0000: shutting down for failover; client state will be preserved.
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff88007610c130
Lustre: MGS has stopped.
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078d6e9b8
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078fc0af0
Write to readonly device loop0 (0x700000) bi_flags: f000000000000001, bi_vcnt: 1, bi_idx: 0, bi->size: 4096, bi_cnt: 2, bi_private: ffff880078d6e9b8
Removing read-only on unknown block (0x700000)
Lustre: server umount lustre-MDT0000 complete
LDISKFS-fs warning (device loop0): ldiskfs_fill_super: extents feature not enabled on this filesystem, use tune2fs.
LDISKFS-fs (loop0): recovery complete
LDISKFS-fs (loop0): mounted filesystem with ordered data mode
LDISKFS-fs warning (device loop0): ldiskfs_fill_super: extents feature not enabled on this filesystem, use tune2fs.
LDISKFS-fs (loop0): mounted filesystem with ordered data mode
Lustre: MGS MGS started
LustreError: 166-1: MGC10.3.0.187@tcp: Connection to service MGS via nid 0@lo was lost; in progress operations using this service will fail.
Lustre: MGC10.3.0.187@tcp: Reactivating import
Lustre: Skipped 1 previous similar message
Lustre: 6254:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import MGS->NET_0x9000000000000_UUID netid 90000: select flavor null
Lustre: 6254:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 11 previous similar messages
Lustre: Enabling ACL
Lustre: Enabling user_xattr
Lustre: lustre-MDT0000: used disk, loading
Lustre: 6257:0:(ldlm_lib.c:1900:target_recovery_init()) RECOVERY: service lustre-MDT0000, 2 recoverable clients, last_transno 8589934644
LustreError: 6261:0:(ldlm_lib.c:1737:target_recovery_thread()) lustre-MDT0000: started recovery thread pid 6261
Lustre: 6257:0:(mdt_lproc.c:259:lprocfs_wr_identity_upcall()) lustre-MDT0000: identity upcall set to /usr/sbin/l_getidentity
Lustre: 6257:0:(mds_lov.c:1004:mds_notify()) MDS mdd_obd-lustre-MDT0000: add target lustre-OST0000_UUID
Lustre: 6257:0:(mds_lov.c:1004:mds_notify()) Skipped 1 previous similar message
Lustre: 2303:0:(ldlm_lib.c:802:target_handle_connect()) lustre-OST0000: received new MDS connection from NID 0@lo, removing former export from same NID
Lustre: 2303:0:(filter.c:2710:filter_connect_internal()) lustre-OST0000: Received MDS connection for group 0
Lustre: 2303:0:(filter.c:2710:filter_connect_internal()) Skipped 1 previous similar message
Lustre: 1629:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0000_UUID
Lustre: 1629:0:(mds_lov.c:1024:mds_notify()) Skipped 1 previous similar message
Lustre: 6293:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 6293:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 5 previous similar messages
Lustre: 2303:0:(ldlm_lib.c:802:target_handle_connect()) lustre-OST0001: received new MDS connection from NID 0@lo, removing former export from same NID
LustreError: 6263:0:(mdt_handler.c:2785:mdt_recovery()) operation 400 on unconnected MDS from 12345-0@lo
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-107)  req@ffff880076ef7800 x1396559449162476/t0(0) o400-><?>@<?>:0/0 lens 192/0 e 0 to 0 dl 1331863092 ref 1 fl Interpret:H/0/ffffffff rc -107/-1
Lustre: lustre-OST0001: haven't heard from client 7612ae0a-d05d-f960-7180-713116b3533a (at 0@lo) in 51 seconds. I think it's dead, and I am evicting it. exp ffff880077c67000, cur 1331863067 expire 1331863037 last 1331863016
Lustre: lustre-OST0000: haven't heard from client 7612ae0a-d05d-f960-7180-713116b3533a (at 0@lo) in 51 seconds. I think it's dead, and I am evicting it. exp ffff880077c63000, cur 1331863067 expire 1331863037 last 1331863016
LustreError: 11-0: an error occurred while communicating with 0@lo. The obd_ping operation failed with -107
Lustre: lustre-MDT0000-mdc-ffff880037ae7800: Connection to service lustre-MDT0000 via nid 0@lo was lost; in progress operations using this service will wait for recovery to complete.
Lustre: 6263:0:(ldlm_lib.c:2026:target_queue_recovery_request()) Next recovery transno: 8589934645, current: 8589934645, replaying
Lustre: 6263:0:(ldlm_lib.c:2026:target_queue_recovery_request()) Skipped 104 previous similar messages
Lustre: setting import lustre-MDT0000_UUID INACTIVE by administrator request
LustreError: 1629:0:(client.c:2530:ptlrpc_replay_interpret()) request replay timed out, restarting recovery
LustreError: 1629:0:(client.c:1055:ptlrpc_import_delay_req()) @@@ invalidate in flight  req@ffff880070e8bc00 x1396559449162481/t0(0) o38->lustre-MDT0000-mdc-ffff880037ae7800@0@lo:12/10 lens 368/512 e 0 to 0 dl 0 ref 1 fl Rpc:N/0/ffffffff rc 0/-1
Lustre: client ffff880037ae7800 umount complete
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: 6263:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from c0f21b44-ed26-7b8b-59e5-2eada293cad7@0@lo recovering/t0 exp (null) cur 1331863069 last 0
Lustre: 6263:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 6 previous similar messages
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 82s
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff8800785ebc00 x1396559449162486/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863089 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 77s
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880076eef800 x1396559449162491/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863094 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 72s
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880079ac4050 x1396559449162495/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863099 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 67s
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880037cdbc00 x1396559449162499/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863104 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 62s
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 57s
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff88007764cc00 x1396559449162507/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863114 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) Skipped 1 previous similar message
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: Skipped 1 previous similar message
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 47s
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) Skipped 1 previous similar message
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880077d83000 x1396559449162523/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863134 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) Skipped 3 previous similar messages
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: Skipped 3 previous similar messages
Lustre: lustre-OST0001: haven't heard from client 8572f9f1-5f2d-6ad7-1efd-6be0b93d382e (at 0@lo) in 57 seconds. I think it's dead, and I am evicting it. exp ffff88007719c000, cur 1331863124 expire 1331863094 last 1331863067
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 27s
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) Skipped 3 previous similar messages
Lustre: 6263:0:(ldlm_lib.c:873:target_handle_connect()) lustre-MDT0000: connection from c0f21b44-ed26-7b8b-59e5-2eada293cad7@0@lo recovering/t0 exp (null) cur 1331863134 last 0
Lustre: 6263:0:(ldlm_lib.c:873:target_handle_connect()) Skipped 12 previous similar messages
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) @@@ processing error (-16)  req@ffff880077e5d400 x1396559449162545/t0(0) o38-><?>@<?>:0/0 lens 368/264 e 0 to 0 dl 1331863169 ref 1 fl Interpret:/0/0 rc -16/0
LustreError: 6263:0:(ldlm_lib.c:2125:target_send_reply_msg()) Skipped 6 previous similar messages
LustreError: 11-0: an error occurred while communicating with 0@lo. The mds_connect operation failed with -16
LustreError: Skipped 6 previous similar messages
Lustre: 6261:0:(ldlm_lib.c:1566:target_recovery_overseer()) recovery is timed out, evict stale exports
LustreError: 6261:0:(genops.c:1270:class_disconnect_stale_exports()) lustre-MDT0000: disconnect stale client 07ae78f5-2f83-cc97-9bb3-c24019145e36@<unknown>
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) lustre-MDT0000: denying connection for new client 0@lo (c0f21b44-ed26-7b8b-59e5-2eada293cad7): 1 clients in recovery for 22s
LustreError: 6263:0:(ldlm_lib.c:906:target_handle_connect()) Skipped 6 previous similar messages
Lustre: 6261:0:(ldlm_lib.c:1566:target_recovery_overseer()) recovery is timed out, evict stale exports
LustreError: 6261:0:(genops.c:1270:class_disconnect_stale_exports()) lustre-MDT0000: disconnect stale client 8572f9f1-5f2d-6ad7-1efd-6be0b93d382e@0@lo
Lustre: lustre-MDT0000: sending delayed replies to recovered clients
Lustre: 6261:0:(mds_lov.c:1024:mds_notify()) MDS mdd_obd-lustre-MDT0000: in recovery, not resetting orphans on lustre-OST0000_UUID
Lustre: 6261:0:(mds_lov.c:1024:mds_notify()) Skipped 1 previous similar message
Lustre: lustre-OST0001: received MDS connection from 0@lo
Lustre: Skipped 1 previous similar message
Lustre: 2303:0:(lustre_log.h:471:llog_group_set_export()) lustre-OST0001: export for group 0 is changed: 0xffff880070e8b400 -> 0xffff880077ddfc00
Lustre: 2303:0:(lustre_log.h:471:llog_group_set_export()) Skipped 2 previous similar messages
Lustre: 2303:0:(llog_net.c:168:llog_receptor_accept()) changing the import ffff880078a43000 - ffff880079e43000
Lustre: 2303:0:(llog_net.c:168:llog_receptor_accept()) Skipped 2 previous similar messages
Lustre: 2304:0:(filter.c:2566:filter_llog_connect()) lustre-OST0000: Recovery from log 0x1f/0x0:be68561c
Lustre: MDS mdd_obd-lustre-MDT0000: lustre-OST0000_UUID now active, resetting orphans
Lustre: Skipped 1 previous similar message
Lustre: 2304:0:(filter.c:2566:filter_llog_connect()) lustre-OST0001: Recovery from log 0x20/0x0:be68561d
Lustre: 6263:0:(sec.c:1474:sptlrpc_import_sec_adapt()) import lustre-MDT0000->NET_0x9000000000000_UUID netid 90000: select flavor null
Lustre: 6263:0:(sec.c:1474:sptlrpc_import_sec_adapt()) Skipped 6 previous similar messages
Lustre: Client lustre-client has started
Lustre: 6633:0:(debug.c:326:libcfs_debug_str2mask()) You are trying to use a numerical value for the mask - this will be deprecated in a future release.
Lustre: 6633:0:(debug.c:326:libcfs_debug_str2mask()) Skipped 1 previous similar message
LustreError: 152-6: Ignoring deprecated mount option 'acl'.
Lustre: DEBUG MARKER: == replay-dual replay-dual.sh test complete, duration 151 sec == 03:59:46 (1331863186)
LustreError: 6866:0:(ldlm_request.c:1172:ldlm_cli_cancel_req()) Got rc -108 from cancel RPC: canceling anyway
LustreError: 6866:0:(ldlm_request.c:1799:ldlm_cli_cancel_list()) ldlm_cli_cancel_list: -108
Lustre: client ffff880079abec00 umount complete
